{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section focuses on setting up our environment. We'll import the required Python packages for our analysis and load the datasets we'll be working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Using Pandas, load the data into 2 csv files, 'train_df' and 'test_df'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'train_df': This dataset will be used for Exploratory Data Analysis (EDA), data wrangling, and building our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_df': This is a separate dataset containing the same predictor variables as the training data but excludes the target variable, 'Survival'. After training our model we'll use this dataset to make predictions on passenger survival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable | Definition | Key |\n",
    "|---|---|---|\n",
    "| survival | Survival | 0 = No, 1 = Yes |\n",
    "| Pclass | Ticket Class | 1 = 1st, 2 = 2nd, 3 = 3rd |\n",
    "| Sex | Sex | - |\n",
    "| Age | Age in years | - |\n",
    "| Sibsp | # of siblings / spouses aboard the Titanic | - |\n",
    "| Parch | # of parents / children aboard the Titanic | - |\n",
    "| Ticket | Ticket Number | - |\n",
    "| Fare | Passenger Fare | - |\n",
    "| Cabin | Cabin Number | - |\n",
    "| Embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1 - Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll dive into Exploratory Data Analysis (EDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 5 rows of train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some general information about our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics on variables with type 'object'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics on variables with type 'number'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of null values in each column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate analysis examines one variable at a time to uncover its individual characteristics. This involves analysing its distribution, including central tendency, spread, and shape, and is often visualised through histograms and box plots. Summary statistics like mean and standard deviation provide a numerical overview of the variable's distribution, laying the groundwork for understanding individual data components before exploring relationships between multiple variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis to understand individual characteristics of attributes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the groupby() function to calculate the survival counts based on sex and load into a new dataframe\n",
    "\n",
    "# Use pandas' loc[] functionality on this dataframe to extract the number of males who survived\n",
    "\n",
    "# Use pandas' loc[] functionality on this dataframe to extract the number of females who survived\n",
    "\n",
    "# Print the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a searborn countplot to display survival counts by sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Use sns.set_style() to set a whitegrid style\n",
    "\n",
    "# Create a countplot using sns.countplot\n",
    "\n",
    "# Label the x and y axis, and add a litle\n",
    "\n",
    "# Add a legend\n",
    "\n",
    "# Display the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "P Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the groupby function to calculate the survival counts based on passenger class and load into a new dataframe\n",
    "\n",
    "# Use pandas' loc[] functionality to extract the number of passengers who survived for each class\n",
    "\n",
    "# Print the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seaborn plot showing survival counts by class\n",
    "\n",
    "# Create a countplot using sns.countplot()\n",
    "\n",
    "# label the x and y axes, add a title\n",
    "\n",
    "# Add legend for clarity\n",
    "\n",
    "# Display the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seaborn histogram to display the distribution of age amongst passengers\n",
    "\n",
    "# Create a histogram with Kernal Density Estimation (kde) of passenger ages using sns.histplot()\n",
    "\n",
    "# label x and y axes, add a title\n",
    "\n",
    "# Display the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots using plt.subplots(), in a 1 by 2 configuration\n",
    "\n",
    "# In the first subplot, create a histogram for those who did not survive\n",
    "\n",
    "# Label the x and y axes, add a title and limit the y axis to be between 0 and 120\n",
    "\n",
    "# In the second subplot, create a histogram for those who survived\n",
    "\n",
    "# Label the x and y axes, add a title and limit the y axis to be between 0 and 120\n",
    "\n",
    "#Improve layout using tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Multivariate Analysis (additional task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate analysis explores relationships between multiple variables simultaneously to uncover complex patterns and interactions. Techniques like correlation analysis help understand variable dependencies and predict outcomes based on multiple factors. By going beyond single-variable analysis, it provides a more comprehensive understanding of datasets and supports better decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set the Seaborn style using sns.set_style, read the documentation to pick a style that you like\n",
    "\n",
    "# Set figure size for better readability - try 10 by 8\n",
    "\n",
    "# Calculate correlation matrix (using numeric_only for clarity) using .corr()\n",
    "\n",
    "# Create a mask for the upper triangle using np.triu()\n",
    "\n",
    "# Choose a diverging color palette\n",
    "\n",
    "# Plot the correlartion matrix using sns.heatmap()\n",
    "\n",
    "# Add title for context\n",
    "\n",
    "# Improve layout using plt.tight_layout() and the show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments on how to interpret results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2 - Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data wrangling encompasses the crucial process of cleaning, transforming, and preparing raw data to make it suitable for effective data analysis and modeling. It's the essential bridge between messy, real-world data and meaningful insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preserve the original data, create a clone of the dataset before performing any data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clone of 'train_df' called 'train_df_preprocessed'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deletion: You can delete rows with missing values. This is a simple approach, but it can reduce the size of your dataset and introduce bias if the missing values are not randomly distributed.\n",
    "\n",
    "Imputation: You can replace missing values with estimated values. There are many different imputation methods, including:\n",
    "\n",
    "Mean/median/mode imputation: Replacing missing values with the mean, median, or mode of the non-missing values in the same column.\n",
    "\n",
    "Regression imputation: Using a regression model to predict the missing values based on the other variables in the dataset.\n",
    "\n",
    "K-nearest neighbors (KNN) imputation: Finding the k most similar rows to the row with the missing value and using the average of their values to impute the missing value.\n",
    "\n",
    "Using algorithms that support missing values: Some machine learning algorithms can handle missing values without any preprocessing. For example, decision trees can be used to handle missing values by creating separate branches for rows with and without the missing value.\n",
    "\n",
    "The best approach for handling missing values will depend on the specific dataset and the goals of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .isnull() and .sum() methods to display null values \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness is a measure of the asymmetry of a distribution. A symmetrical distribution has a skewness of 0. A distribution that is skewed to the right (i.e., has a longer tail on the right side) has a positive skewness, and a distribution that is skewed to the left (i.e., has a longer tail on the left side) has a negative skewness.\n",
    "\n",
    "Mean imputation: Suitable for datasets with a relatively symmetrical distribution, typically when the skewness falls within the range of -0.5 to +0.5. Mean imputation is sensitive to outliers, meaning that extreme values can significantly influence the mean.\n",
    "\n",
    "Median imputation: More appropriate for datasets with a skewed distribution (either positive or negative), generally when the skewness is outside the range of -0.5 to +0.5. The median is less affected by outliers compared to the mean.\n",
    "\n",
    "For age we will plot the skewness, this will help us determine if we will be using the mean or median for imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a visually appealing style using sns.set_style\n",
    "\n",
    "# Set the matplotlib figure using plt.figure()\n",
    "\n",
    "# Create the histogram with KDE using seaborn\n",
    "\n",
    "# Set plot title and labels\n",
    "\n",
    "# Calculate skewness\n",
    "\n",
    "# Display skewness value on the plot u)sing plt.text(\n",
    "\n",
    "# Display the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is only a moderate skew, we will use mean imputation for the age column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the age column\n",
    "\n",
    "# Using the mean age, impute missing age values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the majority of the cabin data is missing, we will drop this column for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop cabin column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embarked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since only 2 embarked values are missing, we will drop these rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with null values in embarked column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding categorical variables is a crucial step in preparing data for machine learning algorithms, as these algorithms typically work with numerical data. Here's a breakdown of common encoding methods:\n",
    "\n",
    "1. Nominal Data (Unordered Categories):\n",
    "\n",
    "One-Hot Encoding:\n",
    "\n",
    "Creates binary (0 or 1) columns for each unique category.\n",
    "\n",
    "Suitable for variables where there's no inherent order.\n",
    "\n",
    "Example: For \"Color\" (Red, Blue, Green), you'd have three columns: \"Color_Red,\" \"Color_Blue,\" \"Color_Green.\"\n",
    "\n",
    "Hashing Encoding:\n",
    "\n",
    "Uses a hash function to convert categories into numerical values.\n",
    "\n",
    "Can be more memory-efficient for high-cardinality variables (many unique categories).\n",
    "\n",
    "2. Ordinal Data (Ordered Categories):\n",
    "\n",
    "Ordinal Encoding (Label Encoding):\n",
    "\n",
    "Assigns a unique integer to each category based on its order.\n",
    "\n",
    "Preserves the order information, which can be beneficial for some algorithms.\n",
    "\n",
    "Example: For \"Size\" (Small, Medium, Large), you might assign 1, 2, and 3 respectively.\n",
    "\n",
    "Target Encoding (Mean Encoding):\n",
    "\n",
    "Replaces each category with the mean target value (e.g., average sales) for that category.\n",
    "\n",
    "Can be powerful but prone to overfitting, especially with small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print column names with categorical values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of unique values in each of these columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this training, we will drop the ticket and name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Ticket and Name columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all unique values in the Sex and Embarked columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this training, we will use label encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the .map() function, map the values in Sex and Embarked columns to integer values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View values in 'Sex' and 'Embarked' columns to ensure changes have been made\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Task - Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age Buckets \n",
    "\n",
    "The task below focuses on feature engineering for exploratory data analysis (EDA). We will transform the numerical 'Age' feature into categorical age groups to potentially uncover hidden patterns and gain a deeper understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define age bins and corresponding labels\n",
    "\n",
    "# Create the 'Age_Group' column using pd.cut()\n",
    "\n",
    "# Create a seaborn count plot to visualise this new feature\n",
    "\n",
    "# Create the count plot\n",
    "\n",
    "# Add labels and title\n",
    "\n",
    "# Rotate x-axis labels for better readability (if needed)\n",
    "\n",
    "# Show the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3 - Logistic Regression: Model Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first five rows of train_df_preprocessed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A train-test split is a common technique used in machine learning to evaluate the performance of a model. It involves dividing the dataset into two separate subsets (generally a 70/30 split): Â  \n",
    "\n",
    "Training set: This subset is used to train the model. The algorithm learns patterns and relationships from this data.\n",
    "\n",
    "Testing set: This subset is held back and used to evaluate the model's performance after training. It helps assess how well the model generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View data to ensure all necessary wrangling is complete\n",
    "\n",
    "# Gender should be assigned a value of 1 or 0, and there should be no null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into X (Predictor variables) and y (Target Variable) - this is the 'Survived' column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test data with a 70/30 split, set the random_state to be 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "\n",
    "# Train the model on the training data using .fit()\n",
    "\n",
    "# Make predictions on the test data using .predict()\n",
    "\n",
    "# Calculate the accuracy of the model on the training data using .score()\n",
    "\n",
    "# Display the accuracy score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Initisalise confusion matrix\n",
    "\n",
    "# Using seaborn a visually appealing heatmap to display this information, try using sns.heatmap()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) Testing Model on test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the identical data preparation steps used on the train_df to the test_df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle submission requirements: CSV file with 2 columns: PassengerId and Survived with 418 rows of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the initial dataset already contains 418 rows, we cannot remove any rows in our preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a clone of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clone of 'test_df' called 'test_df_preprocessed'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We'll apply the same preprocessing steps outlined in Section 2 to the test data, ensuring consistency in data preparation between the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the mean age, impute missing age values\n",
    "\n",
    "# Drop cabin column\n",
    "\n",
    "# Replace null value in fare column with mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Ticket and Name columns\n",
    "\n",
    "# Map values in Sex and Embarked columns to integer values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4) Make predictions on test_df_preprocessed and export output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test_df_preprocessed using our trained logistic regression model\n",
    "\n",
    "# Create Dataframe 'output' with the Passenger ID and the coresponding 'Survived' value\n",
    "\n",
    "# Export this DataFrame as a csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once csv file is created please flag a facilitator to submit for you"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
